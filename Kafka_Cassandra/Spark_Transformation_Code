# Starting the Spark-Shell

sbin/start-all.sh

#Starting the Spark-shell Iwith the new setup
bin/spark-shell --packages "com.datastax.spark:spark-cassandra-connector_2.11:2.0.2","org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0"

#Inside the Spark-Shell
//This code is for spark shell 
spark.stop

import org.apache.spark.SparkConf

import org.apache.spark.SparkContext._

import org.apache.spark.streaming._

import org.apache.spark.streaming.StreamingContext._

import org.apache.spark.streaming.kafka._



import com.datastax.spark.connector.SomeColumns

import com.datastax.spark.connector.cql.CassandraConnector

import com.datastax.spark.connector.streaming._



val sparkConf = new SparkConf().setAppName("KafkaSparkStreaming").set("spark.cassandra.connection.host", "127.0.0.1")

val ssc = new StreamingContext(sparkConf, Seconds(20))

val topicpMap = "mytopic".split(",").map((_, 1.toInt)).toMap

val lines = KafkaUtils.createStream(ssc, "localhost:2181", "sparkgroup", topicpMap).map(_._2)

lines.print();



lines.map(line => { val arr = line.split(","); (arr(0),arr(1),arr(2),arr(3),arr(4)) }).saveToCassandra("sparkdata", "cust_data", SomeColumns("fname", "lname","url","product","cnt"))

ssc.start

ssc.awaitTermination
